
import os
import pandas as pd
import requests
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import timedelta
import datetime as dt

from run_all import COUNTIES

SPLIT = ['047', '111', '099', '081', '073', '105', '031', '049', '067', '097']


def openet_get_fields_export(fields, start, end, et_too=False,
                             api_key='C:/Users/CND571/Documents/Haugen_Montana_API.txt'):
    """ Uses OpenET API multipolygon export endpoint to get etof data given a Google Earth Engine asset.
    Files will be exported to user's Google Drive; this is dependent on linking OpenET account to a Google account
    (I think).

    :fields: path to gee asset, form of 'projects/cloud_project/assets/asset_filename'
    :start: beginning of period of study, 'YYYY-MM-DD' format
    :end: end of period of study, 'YYYY-MM-DD' format
    :et_too: if True, also download OpenET ensemble ET over same time period and set of fields
    :api_key: path to local .txt file where API key from user's OpenET account is stored. Key is first line in file.
    """

    # Use earth engine and gsutil to upload shapefiles to earth engine and then download resulting files from bucket.

    # This is apparently better, as it makes sure to close the file.
    with open(api_key, 'r') as f:
        api_key = f.readline()

    # key_file = open(api_key, "r")
    # api_key = key_file.readline()

    # set your API key before making the request
    header = {"Authorization": api_key}

    # endpoint arguments
    args = {
        "date_range": [
            start,
            end
        ],
        "interval": "monthly",
        "asset_id": fields,
        "attributes": [
            "FID"
        ],
        "reducer": "mean",
        "model": "Ensemble",
        "variable": "ETof",  # "ETof" or "ET"
        "reference_et": "gridMET",
        "units": "in"
    }

    # query the api
    resp = requests.post(
        headers=header,
        json=args,
        url="https://openet-api-montana-ic5gyecbva-uw.a.run.app/raster/export/multipolygon"
    )
    print(resp.json())
    # response = resp.json()
    # tag = response['name']
    # print(tag)

    if et_too:
        # getting et variable too, in separate file.
        args.update({"variable": "ET"})
        resp = requests.post(
            headers=header,
            json=args,
            url="https://openet-api-montana-ic5gyecbva-uw.a.run.app/raster/export/multipolygon"
        )
        print(resp.json())


def get_all_openet_etof_data():
    """ Run openet_get_fields_export for all MT counties. """
    gee_county_files = ['001', '003', '005', '007', '009', '013', '015', '017', '019', '021', '023', '027', '029',
                        '031a', '031b', '033', '035', '037', '039', '041', '043', '045', '047a', '047b', '049a',
                        '049b', '051', '053', '055', '057', '059', '061', '063', '065', '067a', '067b', '069',
                        '071', '073a', '073b', '075', '077', '079', '081a', '081b', '083', '085', '087', '089',
                        '091', '093', '095', '097a', '097b', '099a', '099b', '101', '103', '105a', '105b', '107',
                        '111a', '111b']
    # '011', '025', and '109' have no fields in 15FEB24 Statewide Irrigation Dataset.
    # Period of record for OpenET final data: 2016, provisional back to 1985.
    # start_por, end_por = "1985-01-01", "2023-12-31"
    # Fetching old and new data separately to prevent timeouts.
    old_start, old_end = "1985-01-01", "2015-12-31"
    new_start, new_end = "2016-01-01", "2023-12-31"
    for i in tqdm(gee_county_files, total=len(gee_county_files)):
        gee_asset = 'projects/ee-hehaugen/assets/SID_15FEB2024/{}'.format(i)
        openet_get_fields_export(gee_asset, old_start, old_end)
        openet_get_fields_export(gee_asset, new_start, new_end)


def rename_etof_downloads(path):
    """ Rename OpenET etof files from 'ensemble_etof_XXXXX.csv' to 'ensemble_etof_XXX_XXXX_XXXX.csv'

    Starts out with a random 5-character suffix generated by OpenET API, then gets renamed with FIPS county code,
    first year in timeseries, and last year in time series. Also handles the large counties with split fields/2-part
    gee assets. This will break once it has successfully been run through.

    path: str, specifies local directory where all etof files are stored.
    """
    # first = {i: True for i in SPLIT}
    for i in os.listdir(path):
        oldpath = os.path.join(path, i)
        file = pd.read_csv(oldpath, index_col='time')
        file.index = pd.to_datetime(file.index)
        srt = file.index.min().year
        end = file.index.max().year
        county = file['FID'].iloc[0][:3]  # Or maybe 'fid'
        if county in SPLIT:
            newname = 'ensemble_etof_pt1_{}_{}_{}.csv'.format(county, srt, end)
            newpath = os.path.join(path, newname)
            if os.path.isfile(newpath):
                newname = 'ensemble_etof_pt2_{}_{}_{}.csv'.format(county, srt, end)
            # if first[county]:
            #     newname = 'ensemble_etof_pt1_{}_{}_{}.csv'.format(county, srt, end)
            #     first[county] = False
            # else:
            #     newname = 'ensemble_etof_pt2_{}_{}_{}.csv'.format(county, srt, end)
        else:
            newname = 'ensemble_etof_{}_{}_{}.csv'.format(county, srt, end)
        newpath = os.path.join(path, newname)
        os.rename(oldpath, newpath)


def concat_etof(path, goal_yr_start=1985, goal_yr_end=2023):
    """ Combine all etof files for each county.

    Best to use if already known that all necessary data is downloaded and stored in the same directory. This will
    cause errors/confusion if split counties haven't had all field sets run and data downloaded in all relevant
    time periods.

    path: str, local directory where etof files are stored.
    goal_yr_start: int, optional, the desired start year for the total period of record.
    goal_yr_end: int, optional, the desired end year for the total period of record.
    """
    counties = {name[-17:-14] for name in os.listdir(path)}
    print(len(counties), counties)
    for i in counties:
        files = [name for name in os.listdir(path) if "_{}".format(i) in name]
        print(len(files), i, files)
        goal_file = "ensemble_etof_all_{}_{}_{}.csv".format(i, goal_yr_start, goal_yr_end)
        if goal_file not in files:
            dfs = [pd.read_csv(os.path.join(path, j)) for j in files]
            goal_df = pd.concat(dfs)
            goal_df.drop_duplicates(inplace=True)
            goal_df['time'] = pd.to_datetime(goal_df['time'])
            srt = goal_df['time'].min().year
            end = goal_df['time'].max().year
            real_file = "ensemble_etof_all_{}_{}_{}.csv".format(i, srt, end)
            if real_file == goal_file:
                # print("Saving combined file for {} County ({})".format(COUNTIES[i], i))
                goal_df.to_csv(os.path.join(path, goal_file))
            else:
                print("Please finish downloading data for {} County ({})".format(COUNTIES[i], i))


def check_etof_data(counties, etof_dir, plot=False):
    """ To be used after openet_get_fields_export to check data quality, and before loading any data into db files.
    """
    bad_counties = 0
    # for i in counties:
    for i in tqdm(counties, total=len(counties)):
        if i not in ['049', '067', '097']:
            # Setting up
            if i in SPLIT:
                path_o1 = os.path.join(etof_dir, "ensemble_etof_pt1_{}_1985_2015.csv".format(i))
                data_o1 = pd.read_csv(path_o1)
                path_o2 = os.path.join(etof_dir, "ensemble_etof_pt2_{}_1985_2015.csv".format(i))
                data_o2 = pd.read_csv(path_o2)
                etof_data1 = pd.concat([data_o1, data_o2])

                path_n1 = os.path.join(etof_dir, "ensemble_etof_pt1_{}_2016_2023.csv".format(i))
                data_n1 = pd.read_csv(path_n1)
                path_n2 = os.path.join(etof_dir, "ensemble_etof_pt2_{}_2016_2023.csv".format(i))
                data_n2 = pd.read_csv(path_n2)
                etof_data2 = pd.concat([data_n1, data_n2])

                etof_data = pd.concat([etof_data1, etof_data2])
            else:
                path1 = os.path.join(etof_dir, "ensemble_etof_{}_1985_2015.csv".format(i))
                etof_data1 = pd.read_csv(path1)
                path2 = os.path.join(etof_dir, "ensemble_etof_{}_2016_2023.csv".format(i))
                etof_data2 = pd.read_csv(path2)
                etof_data = pd.concat([etof_data1, etof_data2])

            etof_data['time'] = pd.to_datetime(etof_data['time'])
            start = etof_data['time'].min()
            end = etof_data['time'].max()
            etof_data['time'] = etof_data['time'] + timedelta(days=14)
            points_exp = pd.date_range(start, end, freq='MS') + timedelta(days=14)
            points_exp = points_exp.to_frame()

            accept = range(4, 10)
            etof_data['mask'] = [1 if d.month in accept else 0 for d in etof_data['time']]
            etof_data_gs = etof_data[etof_data['mask'] == 1]
            etof_data_ds = etof_data[etof_data['mask'] == 0]

            points_exp['mask'] = [1 if d.month in accept else 0 for d in points_exp[0]]
            points_exp_gs = points_exp[points_exp['mask'] == 1].index
            num_points_exp = len(points_exp_gs)

            fields = etof_data['FID'].unique()

            # Finding/reporting weird data in growing season, is all we care about
            missing = {}
            for j in fields:
                num_points_act = len(etof_data[etof_data['FID'] == j])
                if num_points_act != num_points_exp:
                    # print("Mismatch in {} time series: there are {} data points when {} are expected."
                    #       .format(j, num_points_act, num_points_exp))
                    this_field = etof_data_gs[etof_data_gs['FID'] == j]
                    one = set(points_exp_gs)
                    two = set(this_field['time'])
                    missing[j] = list(one.difference(two))
                    # print('Missing points:', missing[j])
                    # print()
            all_miss = sum([len(missing[x]) for x in missing if isinstance(missing[x], list)])
            set_miss = set(val for lis in missing.values() for val in lis)
            print('\n{} {} County ({} fields)'.format(i, COUNTIES[i], len(fields)))
            if len(missing) > 0:
                # print('{} fields with missing months {}'.format(len(missing), missing))
                print('{} fields, {:.2f}% missing months'.format(len(missing), (len(missing)/len(fields))*100))
                # is the above line wrong? If so, why? Are we always missing >= one point in each field?
                print('{} points, {:.2f}% of data missing, over {} unique months: {} \n'
                      .format(all_miss, ((all_miss/(len(fields)*num_points_exp))*100), len(set_miss), set_miss))
                bad_counties += 1
            else:
                print('No errors.')

            ygs = etof_data_gs['etof'].mean()
            yds = etof_data_ds['etof'].mean()

            # Plotting
            if plot:
                plt.figure()
                plt.title("{} {} County ({} fields)".format(i, COUNTIES[i], len(fields)))
                # Highlighting the growing season (April through September, inclusive)
                for j in range(start.year, end.year+1):
                    if j == start.year:
                        plt.axvspan(dt.date(j, 4, 1), dt.date(j, 10, 1), alpha=0.15, zorder=0,
                                    label='Growing Season (Apr-Sep)')
                    else:
                        plt.axvspan(dt.date(j, 4, 1), dt.date(j, 10, 1), alpha=0.15, zorder=0)

                # Quick distribution of etof values in this county's fields over time.
                plt.scatter(etof_data['time'], etof_data['etof'],
                            c='k', alpha=8/len(fields), edgecolors='none', zorder=2)

                # # Plotting individual field time series.
                # for j in fields[:10]:
                #     etof_field = etof_data[etof_data['FID'] == j]
                #     plt.plot(etof_field['time'], etof_field['etof'], label=j[-3:], zorder=3)

                # Plotting missing/additional data, lines get bolder with more fields missing months on the same date.
                first_m = 0
                for k, v in missing.items():
                    if first_m == 0:
                        plt.vlines(v, 0, 1.2, alpha=0.1, color='tab:orange', label='Missing Dates ({})'
                                   .format(all_miss))
                        first_m = 1
                    else:
                        plt.vlines(v, 0, 1.2, alpha=0.1, color='tab:orange')

                # mean growing season vs winter etof
                plt.hlines(ygs, dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                           'tab:orange', zorder=4, label='Growing Season Mean etof: {:.2f}'.format(ygs))
                plt.hlines(yds, dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                           'tab:blue', zorder=4, label='Dormant Season Mean etof: {:.2f}'.format(yds))

                # Formatting stuff
                plt.hlines([0, 1], dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                           'k', zorder=4)
                plt.grid(zorder=1)
                plt.xlim(dt.date(1985, 1, 1), dt.date(2024, 1, 1))
                plt.ylim(0, 1.2)
                plt.ylabel('etof')
                plt.legend()
    print('----------------------------------------')
    print('{}/{} total counties with some weird data.'.format(bad_counties, len(counties)))
    if plot:
        plt.show()


def check_etof_data_concat(counties, etof_dir, plot=False):
    """ To be used after openet_get_fields_export and concat__etof to check data quality, and before loading
    any data into db files.
    """
    bad_counties = 0
    # for i in counties:
    for i in tqdm(counties, total=len(counties)):
        # Setting up
        path = os.path.join(etof_dir, "ensemble_etof_all_{}_1985_2023.csv".format(i))
        etof_data = pd.read_csv(path)
        etof_data['time'] = pd.to_datetime(etof_data['time'])
        start = etof_data['time'].min()
        end = etof_data['time'].max()
        etof_data['time'] = etof_data['time'] + timedelta(days=14)
        points_exp = pd.date_range(start, end, freq='MS') + timedelta(days=14)
        points_exp = points_exp.to_frame()

        accept = range(4, 10)
        etof_data['mask'] = [1 if d.month in accept else 0 for d in etof_data['time']]
        etof_data_gs = etof_data[etof_data['mask'] == 1]
        etof_data_ds = etof_data[etof_data['mask'] == 0]

        points_exp['mask'] = [1 if d.month in accept else 0 for d in points_exp[0]]
        points_exp_gs = points_exp[points_exp['mask'] == 1].index
        num_points_exp = len(points_exp_gs)

        fields = etof_data['FID'].unique()

        # Finding/reporting weird data in growing season, is all we care about
        missing = {}
        for j in fields:
            num_points_act = len(etof_data[etof_data['FID'] == j])
            if num_points_act != num_points_exp:
                # print("Mismatch in {} time series: there are {} data points when {} are expected."
                #       .format(j, num_points_act, num_points_exp))
                this_field = etof_data_gs[etof_data_gs['FID'] == j]
                one = set(points_exp_gs)
                two = set(this_field['time'])
                missing[j] = list(one.difference(two))
                # print('Missing points:', missing[j])
                # print()
        all_miss = sum([len(missing[x]) for x in missing if isinstance(missing[x], list)])
        set_miss = set(val for lis in missing.values() for val in lis)
        print('\n{} {} County ({} fields)'.format(i, COUNTIES[i], len(fields)))
        if len(missing) > 0:
            # print('{} fields with missing months {}'.format(len(missing), missing))
            print('{} fields, {:.2f}% missing months'.format(len(missing), (len(missing)/len(fields))*100))
            # is the above line wrong? If so, why? Are we always missing >= one point in each field?
            print('{} points, {:.2f}% of data missing, over {} unique months: {} \n'
                  .format(all_miss, ((all_miss/(len(fields)*num_points_exp))*100), len(set_miss), set_miss))
            bad_counties += 1
        else:
            print('No errors.')

        ygs = etof_data_gs['etof'].mean()
        yds = etof_data_ds['etof'].mean()

        # Plotting
        if plot:
            plt.figure()
            plt.title("{} {} County ({} fields)".format(i, COUNTIES[i], len(fields)))
            # Highlighting the growing season (April through September, inclusive)
            for j in range(start.year, end.year+1):
                if j == start.year:
                    plt.axvspan(dt.date(j, 4, 1), dt.date(j, 10, 1), alpha=0.15, zorder=0,
                                label='Growing Season (Apr-Sep)')
                else:
                    plt.axvspan(dt.date(j, 4, 1), dt.date(j, 10, 1), alpha=0.15, zorder=0)

            # Quick distribution of etof values in this county's fields over time.
            plt.scatter(etof_data['time'], etof_data['etof'], c='k', alpha=8/len(fields), edgecolors='none', zorder=2)

            # # Plotting individual field time series.
            # for j in fields[:10]:
            #     etof_field = etof_data[etof_data['FID'] == j]
            #     plt.plot(etof_field['time'], etof_field['etof'], label=j[-3:], zorder=3)

            # Plotting missing/additional data, lines get bolder with additional fields missing months on the same date.
            first_m = 0
            for k, v in missing.items():
                if first_m == 0:
                    plt.vlines(v, 0, 1.2, alpha=0.1, color='tab:orange', label='Missing Dates ({})'
                               .format(all_miss))
                    first_m = 1
                else:
                    plt.vlines(v, 0, 1.2, alpha=0.1, color='tab:orange')

            # mean growing season vs winter etof
            plt.hlines(ygs, dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                       'tab:orange', zorder=4, label='Growing Season Mean etof: {:.2f}'.format(ygs))
            plt.hlines(yds, dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                       'tab:blue', zorder=4, label='Dormant Season Mean etof: {:.2f}'.format(yds))

            # Formatting stuff
            plt.hlines([0, 1], dt.date(1985, 1, 1), dt.date(2024, 1, 1),
                       'k', zorder=4)
            plt.grid(zorder=1)
            plt.xlim(dt.date(1985, 1, 1), dt.date(2024, 1, 1))
            plt.ylim(0, 1.2)
            plt.ylabel('etof')
            plt.legend()
    print('----------------------------------------')
    print('{}/{} total counties with some weird data.'.format(bad_counties, len(counties)))
    if plot:
        plt.show()


if __name__ == '__main__':
    # STEP 1: Get the data from Openet to Google Drive (about 36 hours?)
    # get_all_openet_etof_data()

    # STEP 2: Download files manually to path specified below, rename files to something useful.
    path_ = 'C:/Users/CND571/Documents/Data/etof_files/old-copy'  # old-copy has concat files, old has separate.
    # rename_etof_downloads(path_)  # Breaks when all files have been renamed already.

    # STEP 3: Data quality checks (about 30 mins)
    # concat_etof(path_)  # run in duplicate directory
    for key in ['011', '025', '109']:
        COUNTIES.pop(key, None)
    cnty = list(COUNTIES.keys())
    # cnty = ['049', '067', '097']  # subset
    check_etof_data_concat(cnty, path_, True)
# ========================= EOF ====================================================================
